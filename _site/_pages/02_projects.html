<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Tanmay Parekh</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/_pages/02_projects.html">
  <link rel="alternate" type="application/rss+xml" title="Tanmay Parekh" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/"><b>Tanmay Parekh</b></a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/publications/">Publications</a>
            
          
            
            
          
            
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title"></h1>
  </header>

  <div class="post-content">
    <!-- ---
layout: page
title: Research Projects
permalink: /projects/
---
<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>

___

# Data-Driven Deep Reinforcement Learning
One of the primary factors behind the success of machine learning approaches in open world settings, such as [image recognition](https://arxiv.org/abs/1512.03385) and [natural language processing](https://arxiv.org/abs/1810.04805), has been the ability of high-capacity deep neural network function approximators to learn generalizable models from large amounts of data. Deep reinforcement learning methods, however, require active online data collection, where the model actively interacts with its environment. This makes such methods hard to scale to complex real-world problems, where active data collection means that large datasets of experience must be collected for every experiment -- this can be expensive and, for systems such as autonomous vehicles or robots, potentially [unsafe](https://arxiv.org/abs/1801.08757). In a number of domains of practical interest, such as autonomous driving, robotics, and games, there exist plentiful amounts of previously collected interaction data which, consists of informative behaviours that are a rich source of prior information. Deep RL algorithms that can utilize such prior datasets will not only scale to real-world problems, but will also lead to solutions that generalize substantially better. A **data-driven** paradigm for reinforcement learning will enable us to pre-train and deploy agents capable of sample-efficient learning in the real-world.  

![](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575315128671_off_policy_teaser.png)


In this work, we ask the following question: *Can deep RL algorithms effectively leverage prior collected offline data and learn without interaction with the environment?* ******We refer to this problem statement as *fully off-policy RL*, previously also called *batch RL* in literature*.* A class of deep RL algorithms, known as off-policy RL algorithms can, in principle, learn from previously collected data. Recent off-policy RL algorithms such as [Soft Actor-Critic](https://arxiv.org/abs/1812.05905) (SAC), [QT-Opt](https://arxiv.org/abs/1806.10293), and [Rainbow](https://arxiv.org/abs/1710.02298), have demonstrated sample-efficient performance in a number of challenging domains such as robotic manipulation and atari games. However, all of these methods still require online data collection, and their ability to learn from fully off-policy data is limited in practice. In this work, we show why existing deep RL algorithms can fail in the fully off-policy setting. We then propose effective solutions to mitigate these issues.

## Why can't off-policy deep RL algorithms learn from static datasets?    

Let's first study how state-of-the-art deep RL algorithms perform in the fully off-policy setting. We choose the Soft Actor-critic (SAC) algorithm and investigate its performance in the fully off-policy setting. Figure 1 shows the training curve for SAC trained solely on varying amounts of previously collected *expert demonstrations* for the HalfCheetah-v2 gym benchmark task. Although the data demonstrates successful task completion, none of these runs succeed, with corresponding Q-values diverging in some cases. At first glance, this resembles ***overfitting**,* as the evaluation performance deteriorates with more training (green curve), but increasing the size of the static dataset does not rectify the problem (orange (1e4 samples) vs green (1e5 samples)) , suggesting the issue is more complex. 

![Figure 1: Average Return and logarithm of the Q-value for HalfCheetah-v2 with varying amounts of expert data](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575315719190_figure1_problem_exists.png)


Most off-policy RL methods in use today, including SAC used in Figure 1, are based on approximate dynamic programming (though many also utilize importance sampled policy gradients, for example [Liu et al. (2019)](http://auai.org/uai2019/proceedings/papers/440.pdf)). The core component of approximate dynamic programming in deep RL is the value function or Q-function. The *optimal* Q-function $$Q^*$$ obeys the optimal Bellman equation, given below:

                $$Q^* = \mathcal{T}^* Q^* \text{~~;~~} (\mathcal{T}^* \hat{Q})(s, a) := R(s, a) + \gamma \mathbb{E}_{T(s'|s,a)}[\max_{a'}\hat{Q}(s', a')]$$

Reinforcement learning then corresponds to minimizing the squared difference between the left-hand side and right-hand side of this equation, also referred to as the mean squared Bellman error (MSBE): 

                    $$Q := \arg \min_{\hat{Q}} \mathbb{E}_{s \sim \mathcal{D}, a \sim \beta(a|s)} \left[ (\hat{Q}(s, a) - (\mathcal{T}^* \hat{Q})(s, a))^2 \right]$$

MSBE is minimized on transition samples in a dataset $$\mathcal{D}$$ generated by a behavior policy $$\beta(a|s)$$. 
Although minimizing MSBE corresponds to a supervised regression problem, the targets for this regression are themselves derived from the current Q-function estimate. 

We can understand the source of the instability shown in Figure 1 by examining the form of the Bellman backup used to train a Q-function.  The targets are calculated by maximizing the learned Q-values with respect to the action at the next state ($$s'$$) for Q-learning methods, or by computing an expected value under the policy at the next state, $$\mathbb{E}_{s' \sim T(s'|s, a), a' \sim \pi(a'|s')} [\hat{Q}(s', a')]$$ for actor-critic methods that maintain an explicit policy alongside a Q-function. However, the Q-function estimator is only reliable on action-inputs from the behavior policy $$\beta$$, which is the training distribution. As a result, naively maximizing the value may evaluate the $$\hat{Q}$$ estimator on actions that lie far outside of the training distribution, resulting in pathological values that incur large absolute error from the optimal desired Q-values. We refer to such actions as out-of-distribution (OOD) actions. We call the error in the Q-values caused due to backing up target values corresponding to OOD actions during backups as bootstrapping error. A schematic of this problem is shown in Figure 2 below.

![Figure 2: Incorrectly high Q-values for OOD actions may be used for backups, leading to accumulation of error.](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575316706286_problem_Q-values.png)


Using an incorrect Q-value for computing the backup leads to accumulation of error -- minimizing MSBE perfectly leads to propagation of all imperfections in the target values into the current Q-function estimator. We refer to this process as accumulation or **propagation** of bootstrapping error over iterations of training. The agent is unable to correct errors as it is unable to gather ground truth return information by actively exploring the environment. Accumulation of bootstrapping error can make Q-values diverge to infinity (for example, blue and green curves in Figure 1), or not converge to the correct values (for example, red curve in Figure 1), leading to final performance that is substantially worse than even the average performance in the dataset.

## Towards deep RL algorithms that learn in the presence of OOD actions

*How can we develop RL algorithms that learn from static data without being affected by OOD actions?* We will first review some of the existing approaches in literature towards solving this problem and then describe our recent work, BEAR which tackles this problem. There are broadly two classes of methods towards solving this problem. 

- **Behavioral cloning (BC) based methods:** When the static dataset is generated by an expert, one can utilize behavioral cloning to mimic the expert policy, as is done in imitation learning. In a generalized setting, where the behavior policy can be suboptimal but reward information is accessible, one can choose to only mimic a subset of good action decisions from the entire dataset. This is the idea behind prior works such as [reward-weighted regression](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/ICML2007-Peters_4493[0].pdf) (RWR), [relative entropy policy search](https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264) (REPS), [MARWIL](https://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data), some very recent works such as [ABM](https://openreview.net/forum?id=rke7geHtwH), and a recent work from our lab, called [advantage weighted regression](https://arxiv.org/abs/1910.00177) (AWR) where we show that advantage-weighted form of behavioral cloning, which assigns higher likelihoods to demonstration actions that receive higher advantages, can also be used in such situations. Such a method trains only on actions observed in the dataset, hence avoids OOD actions completely. 
- **Dynamic programming (DP) methods:** Dynamic programming methods are appealing in fully off-policy RL scenarios because of their ability to pool information across trajectories, unlike BC-based methods that are implicitly constrained to lie in the vicinity of the best performing trajectory in the static dataset. For example, Q-iteration on a dataset consisting of all transitions in an MDP should return the optimal policy at convergence, however previously described BC-based methods may fail to recover optimality if the individual trajectories are highly suboptimal. ****Within this class, some recent work includes [batch constrained Q-learning](https://arxiv.org/abs/1812.02900) (BCQ) that constrains the trained policy distribution to lie close to the behavior policy that generated the dataset. This is an optimal strategy when the static dataset is generated by an expert policy. However, this might be suboptimal if the data comes from an arbitrarily suboptimal policy. Other recent work, [G-Learning](https://arxiv.org/abs/1512.08562), [KL-Control](https://arxiv.org/abs/1907.00456), [BRAC](https://arxiv.org/abs/1911.11361) implements closeness to the behavior policy by solving a KL-constrained RL problem. [SPIBB](https://arxiv.org/abs/1712.06924), selectively constrains the learned policy to match the behavior policy in probability density on less frequent actions.

The key question we pose in our work is: **Which policies can be reliably used for backups without backing up OOD actions?** Once this question is answered, the job of an RL algorithm reduces to picking the best policy in this set. In our work, we provide a theoretical characterization of this set of policies and use insights from theory to propose a practical dynamic programming based deep RL algorithm called [BEAR](https://arxiv.org/abs/1906.00949) that learns from purely static data.

## Bootstrapping Error Accumulation Reduction (BEAR)
![Figure 3: Illustration of support constraint (BEAR) (right) and distribution-matching constraint (middle)](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575317259331_distinction_methods.png)


The key idea behind BEAR is to constrain the learned policy to lie *within the support* (Figure 3, right) of the behavior policy distribution. This is in contrast to distribution matching (Figure 3, middle) â BEAR does not constrain the learned policy to be close in distribution to the behavior policy, but only requires that the learned policy places non-zero probability mass on actions with non-negligible behavior policy density. We refer to this as **support constraint.** As an example, in a setting with a uniform-at-random behavior policy, a support constraint allows dynamic programming to learn an optimal, deterministic policy. However, a distribution-matching constraint will require that the learned policy be highly stochastic (and thus not optimal), for instance in Figure 3, middle, the learned policy is constrained to be one of the stochastic purple policies, however in Figure 3, right, the learned policy can be a (near-)deterministic yellow policy. For the readers interested in theory, the theoretical insight behind this choice is that a support constraint enables us to control error propagation by upper bounding [concentrability](https://www.aaai.org/Papers/AAAI/2005/AAAI05-159.pdf) under the learned policy, while providing the capacity to reduce divergence from the optimal policy.   

How do we enforce that the learned policy satisfies the support constraint? In practice, we use the sampled [Maximum Mean Discrepancy](http://jmlr.csail.mit.edu/papers/v13/gretton12a.html) (MMD) distance between actions as a measure of support divergence.  (k is any RBF kernel)
$$\text{MMD}^2(\{x_1, \cdots, x_n\}, \{y_1, \cdots, y_m\}) = \frac{1}{n^2} \sum_{i, i'} k(x_i, x_{i'}) - \frac{2}{nm} \sum_{i, j} k(x_i, y_j) + \frac{1}{m^2} \sum_{j, j'} k(y_j, y_{j'}).$$
A simple code snippet for computing MMD is shown below:

    def gaussian_kernel(x, y, sigma=0.1):
      return exp(-(x - y).pow(2).sum() / (2 * sigma.pow(2)))
    
    def compute_mmd(x, y):
      k_x_x = gaussian_kernel(x, x)
      k_x_y = gaussian_kernel(x, y)
      k_y_y = gaussian_kernel(y, y)
      return sqrt(k_x_x.mean() + k_y_y.mean() - 2*k_x_y.mean())

$$\text{MMD}$$ is amenable to stochastic gradient-based training and we show that computing $$\text{MMD}(P, Q)$$ using only few samples from both distributions $$P$$ and $$Q$$ provides sufficient signal to quantify differences in support but not in probability density, hence making it a preferred measure for implementing the support constraint. To sum up, the new (constrained-)policy improvement step in an actor-critic setup is given by: 

        $$\pi_\phi := \max_{\pi \in \Delta_{|S|}} \mathbb{E}_{s \sim \mathcal{D}} \mathbb{E}_{a \sim \pi(\cdot|s)} \left[ Q_\theta(s, a)\right] \text{~~s.t.~~} \mathbb{E}_{s \sim \mathcal{D}} [\text{MMD}(\beta(\cdot|s), \pi(\cdot|s))] \leq \varepsilon$$

On a side note, a BEAR-like support constraint also allows us to incorporate the best of BC-based methods by introducing advantage weighting by modifying the constraint to $$\mathbb{E}_{s \sim \mathcal{D}} \left[\text{MMD}\left(\beta(\cdot|s) \exp (A_{\beta}(s, a)), \pi(\cdot|s)\right) \right] \leq \varepsilon$$, thus giving us the combined benefits of both categories of methods. 

**Support constraint vs Distribution-matching constraint**
Some works, for example, [KL-Control](https://arxiv.org/abs/1907.00456), [BRAC](https://arxiv.org/abs/1911.11361), [G-Learning](https://arxiv.org/abs/1512.08562), argue that using a distribution-matching constraint might suffice in such fully off-policy RL problems. In this section, we take a slight detour towards deeply analyzing this choice. In particular, we provide instances of MDPs where distribution-matching constraint might lead to arbitrarily suboptimal behavior, while support matching does not in principle suffer from this issue. Distribution matching might 

Consider a 2D-gridworld MDP, where the optimal trajectory dictates starting from state $$S$$, moving to the lower-left corner goal $$G$$ and staying there. The behavior data consists of states and actions indicated in black on the gridworld on bottom-left. In the three rows to the right, we show the learning progress of three methods â vanilla Q-learning, Q-learning with distribution-matching constraint (Policy Evaluation) and Q-learning with support constraint (Support Constrained). Dark values indicate high error, and light values indicate low error. 

![](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575188510158_gridworld.png)


Standard backups (top) propagate large errors from the low-support regions into the high-support regions, by backing up OOD actions. Distribution-matching (bottom) reduces error propagation by not backing up OOD actions, but leads to a suboptimal policy, as the data policy is not optimal. A carefully chosen support-constraint (middle) strikes a balance between these two extremes, by confining error propagation in the low-support region while reducing suboptimality of the learned policy.

More generally, if the distribution-matching constraint is tight, then it might not allow policy improvement and is hence suboptimal, as shown in the example above, whereas if the constraint isn't maintained tightly, then it can lead to backing up OOD actions as in the case of standard backups. Support constraints avoid these issues, as maintaining this constraint tightly prevents backups from OOD actions, while still giving the ability to learn optimal policies.

## So, how does BEAR perform in practice?

In our experiments, we evaluated BEAR on three kinds of datasets generated by -- (1) a partially-trained medium-return policy, (2) a random low-return policy and (3) an expert, high-return policy. (1) resembles the settings in practice â such as autonomous driving or robotics, where offline data is collected via scripted policies for robotic grasping or consists of human driving data (which may not be perfect) respectively. Such data is useful as it demonstrates non-random, but still not optimal behavior and we expect training on offline data to be most useful in this setting. Good performance on *both* (2) and (3) demonstrates that the versatility of an algorithm to arbitrary dataset compositions.

For each dataset composition, we compare BEAR to a number of baselines including BC, BCQ, and deep Q-Learning from demonstrations ([DQfD](https://arxiv.org/abs/1704.03732)). In general, we find that BEAR outperforms the best performing baseline in setting (1), and BEAR is the only algorithm capable successfully learning a better-than-dataset policy in both (2) and (3). We show some learning curves below. BC or BCQ type methods usually do not perform great with random data, partly because of the usage of a distribution-matching constraint as described earlier.

![Performance on (1) medium-quality dataset: BEAR outperforms the best performing baseline in all environments.](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575194663346_Screen+Shot+2019-12-01+at+2.03.50+AM.png)

![Performance on (3) expert data: BEAR recovers the performance in the expert dataset, and performs similarly as other methods such as BC.](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575194824925_Screen+Shot+2019-12-01+at+2.06.48+AM.png)
![Performance on (2) random data: BEAR recovers better than dataset performance, and is close to the best performing algorithm (Naive RL) in this case.](https://paper-attachments.dropbox.com/s_03D8A88577B961181603AE5EDBD4A511CD8E828E7651B8AA640A61950DAB9783_1575194836848_Screen+Shot+2019-12-01+at+2.05.38+AM.png)

## Future Directions and Open Problems

Most of the prior datasets for real-world problems such as [RoboNet](https://www.robonet.wiki/) and [Starcraft replays](https://github.com/TorchCraft/StarData) consist of multimodal behavior generated by different users and robots. Hence, one of the next steps to look at is learning from a diverse mixtures of policies. How can we effectively learn policies from a static dataset that consists of a diverse range of behavior -- possibly interaction from a diverse range of tasks, in the spirit of what we encounter in the real world? This question is mostly unanswered at the moment. Some very recent work, such as [REM](https://arxiv.org/abs/1907.04543), shows that simple modifications to existing distributional off-policy RL algorithms in the Atari domain can enable fully off-policy learning on entire interaction data generated from the training run of a separate DQN agent. However, the best solution for learning from a dataset generated by any arbitrary mixture of policies -- which is more likely the case in practical problems -- is unclear.

A rigorous theoretical characterization of the best achievable policy as a function of a given dataset is also an open problem. In our paper, we analyze this question by looking at typically used assumptions of bounded concentrability in the error and convergence analysis of Fitted Q-iteration. Which other assumptions can be applied to analyze this problem? And which of these assumptions is least restrictive and practically feasible? What is the theoretical optimum of what can be achieved solely by offline training?

We hope that our work, BEAR, takes us a step closer to effectively leveraging the most out of prior datasets in an RL algorithm. A **data-driven** paradigm of RL where one could (pre-)train RL algorithms with large amounts of prior data will enable us to go beyond the active exploration bottleneck, thus giving us agents that can be deployed and keep learning continuously in the real world.   

----------

This blog post is based on the our recent paper:
 
- **Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction**
    Aviral Kumar*, Justin Fu*, George Tucker, Sergey Levine
    *In Advances in Neural Information Processing Systems, 2019* 

The code is available [online](https://github.com/aviralkumar2907/BEAR) and a slide-deck explaining the algorithm is available [here](https://sites.google.com/view/bear-off-policyrl). *I would like to thank Sergey Levine for his valuable feedback on the blog post.*  


 -->

  </div>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">Tanmay Parekh</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Tanmay Parekh
            
            </li>
            
            <li><a href="mailto:tparekh@andrew.cmu.edu">tparekh@andrew.cmu.edu</a></li>
            
            
            <li><a href="mailto:tparekh97@gmail.com">tparekh97@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-4">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/TanmayParekh"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">TanmayParekh</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/tparekh97"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">tparekh97</span></a>

          </li>
          

          
          <li>
            <a href="https://www.linkedin.com/in/tanmayparekh"><span class="icon icon--linkedin"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 50 512 512" style="enable-background:new 0 0 90 90;" xml:space="preserve">
<g>
<path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683 C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615 c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915 s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
</g>
</svg></span><span class="username">&nbsp;Tanmay Parekh</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-1">
        <!-- <p>Homepage of Tanmay Parekh</p> -->
        <p>Homepage of Tanmay Parekh<br>Language Technologies Institute<br>Carnegie Mellon University</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
